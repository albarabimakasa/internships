{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bara.ipynb",
      "provenance": [],
      "mount_file_id": "1P9M8DKhPgC1k6zJKcrKwxMgwe0uXMeJB",
      "authorship_tag": "ABX9TyMhPqixLnAYGOs6h6WL1kLt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/albarabimakasa/internships/blob/main/Bara.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBTU0QokiyDi"
      },
      "source": [
        "# Standard Data Science Libraries\n",
        "import pickle\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "\n",
        "# Neural Net Preprocessing\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# Neural Net Layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "# Neural Net Training\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "from pickle import load\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "iG1bHKWu7_gS",
        "outputId": "11531d46-200e-40ec-edb4-592b10c87376"
      },
      "source": [
        "from google.colab import files\n",
        "myfiles = files.upload()\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-687bbf84-9943-49f0-8040-1afdd2bd38cd\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-687bbf84-9943-49f0-8040-1afdd2bd38cd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving train.csv to train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wEBFrFKKvLX"
      },
      "source": [
        "# Import the data\n",
        "train_df = pd.read_csv('train.csv')\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "-AN8wYyo8_RL",
        "outputId": "a13459d7-eae6-4758-d204-1cc10999029e"
      },
      "source": [
        "train_df"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>author</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id26305</td>\n",
              "      <td>This process, however, afforded me no means of...</td>\n",
              "      <td>EAP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id17569</td>\n",
              "      <td>It never once occurred to me that the fumbling...</td>\n",
              "      <td>HPL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id11008</td>\n",
              "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
              "      <td>EAP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>id27763</td>\n",
              "      <td>How lovely is spring As we looked from Windsor...</td>\n",
              "      <td>MWS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>id12958</td>\n",
              "      <td>Finding nothing else, not even gold, the Super...</td>\n",
              "      <td>HPL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19574</th>\n",
              "      <td>id17718</td>\n",
              "      <td>I could have fancied, while I looked at it, th...</td>\n",
              "      <td>EAP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19575</th>\n",
              "      <td>id08973</td>\n",
              "      <td>The lids clenched themselves together as if in...</td>\n",
              "      <td>EAP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19576</th>\n",
              "      <td>id05267</td>\n",
              "      <td>Mais il faut agir that is to say, a Frenchman ...</td>\n",
              "      <td>EAP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19577</th>\n",
              "      <td>id17513</td>\n",
              "      <td>For an item of news like this, it strikes us i...</td>\n",
              "      <td>EAP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19578</th>\n",
              "      <td>id00393</td>\n",
              "      <td>He laid a gnarled claw on my shoulder, and it ...</td>\n",
              "      <td>HPL</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>19579 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            id                                               text author\n",
              "0      id26305  This process, however, afforded me no means of...    EAP\n",
              "1      id17569  It never once occurred to me that the fumbling...    HPL\n",
              "2      id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
              "3      id27763  How lovely is spring As we looked from Windsor...    MWS\n",
              "4      id12958  Finding nothing else, not even gold, the Super...    HPL\n",
              "...        ...                                                ...    ...\n",
              "19574  id17718  I could have fancied, while I looked at it, th...    EAP\n",
              "19575  id08973  The lids clenched themselves together as if in...    EAP\n",
              "19576  id05267  Mais il faut agir that is to say, a Frenchman ...    EAP\n",
              "19577  id17513  For an item of news like this, it strikes us i...    EAP\n",
              "19578  id00393  He laid a gnarled claw on my shoulder, and it ...    HPL\n",
              "\n",
              "[19579 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4O_tNJ6gK01P",
        "outputId": "32366c94-8edb-45d4-86b2-1bdb9f3187f5"
      },
      "source": [
        "# Selecting Edgar Allen Poe as author style to emulate\n",
        "author = train_df[train_df['author']=='EAP'][\"text\"]\n",
        "print('Number of training sentences: ',author.shape[0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training sentences:  7900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yw2LvL0MeRk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e97abe66-16a0-41b3-fc73-8f2946f78fa4"
      },
      "source": [
        "\n",
        "max_words = 50000 # Max size of the dictionary\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(author.values)\n",
        "sequences = tokenizer.texts_to_sequences(author.values)\n",
        "print(sequences[:5])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[19, 2397, 80, 1001, 29, 31, 177, 2, 4073, 1, 1960, 2, 11, 3024, 15, 7, 110, 157, 41, 2146, 3, 481, 4, 1, 149, 2147, 7, 393, 74, 114, 101, 439, 2, 1, 162, 32, 913, 6453, 136, 1, 380], [6, 21, 142, 150, 10, 5, 551, 2148, 319, 28, 16, 15, 20, 8999, 128, 1, 3025, 2398, 30, 171, 2, 1797, 697, 20, 180, 2148, 6454, 12, 33, 188, 2, 1, 869, 243, 522, 1264], [1, 6455, 203, 14, 19, 149, 180, 6456, 6, 1, 1357, 2, 1358, 9000, 3, 83, 2149, 10, 355, 140, 794], [1, 4074, 491, 6, 9001, 28, 11, 158], [7, 287, 9, 36, 48, 22, 73, 4, 644, 9002, 114, 101, 346, 4, 271, 2, 9003, 3, 81, 2, 1, 3026, 2, 6457, 3, 282, 53, 34, 6458, 19, 339, 22, 43, 97, 608, 7, 450, 4, 36, 133, 1191, 88, 12, 133, 71, 914, 1, 759, 3027, 2, 9, 1445, 1359, 18, 760, 12, 4973, 6, 1, 421, 9004, 9005, 7, 214, 9, 36, 48, 22, 3449, 3028, 98, 124, 1192, 4, 1, 92, 9006, 6, 3450, 3, 7, 761, 870, 9, 36, 55, 111, 32]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kQMkpe8-DXq"
      },
      "source": [
        "# Flatten the list of lists resulting from the tokenization. This will reduce the list\n",
        "# to one dimension, allowing us to apply the sliding window technique to predict the next word\n",
        "text = [item for sublist in sequences for item in sublist]\n",
        "vocab_size = len(tokenizer.word_index)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1MGBnFs-Fw8",
        "outputId": "af71360a-9db9-418c-a540-b7f8a139512f"
      },
      "source": [
        "print('Vocabulary size in this corpus: ', vocab_size)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size in this corpus:  15713\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "387LcYDO-ItC"
      },
      "source": [
        "# Training on 19 words to predict the 20th\n",
        "sentence_len = 20\n",
        "pred_len = 1\n",
        "train_len = sentence_len - pred_len\n",
        "seq = []\n",
        "# Sliding window to generate train data\n",
        "for i in range(len(text)-sentence_len):\n",
        "    seq.append(text[i:i+sentence_len])\n",
        "# Reverse dictionary to decode tokenized sequences back to words\n",
        "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "\n",
        "# Save tokenizer\n",
        "# dump(tok, open('tokenizer.pkl', 'wb'))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoTthDoI-L-U"
      },
      "source": [
        "# Each row in seq is a 20 word long window. We append he first 19 words as the input to predict the 20th word\n",
        "trainX = []\n",
        "trainy = []\n",
        "for i in seq:\n",
        "    trainX.append(i[:train_len])\n",
        "    trainy.append(i[-1])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES9EfeSC-PsS"
      },
      "source": [
        "# define model\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size+1, 50, input_length=train_len),\n",
        "    LSTM(100, return_sequences=True),\n",
        "    LSTM(100),\n",
        "    Dense(100, activation='relu'),\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwk98Yau-SUT",
        "outputId": "7b57c38b-b050-455e-9f64-55b2acb5c6f1"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 19, 50)            785700    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 19, 100)           60400     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 15713)             1587013   \n",
            "=================================================================\n",
            "Total params: 2,523,613\n",
            "Trainable params: 2,523,613\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0PFbgf9-U4Q",
        "outputId": "55c6e0ed-85b6-42cc-9a17-b6c072dd8971"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(np.asarray(trainX),\n",
        "         pd.get_dummies(np.asarray(trainy)),\n",
        "         epochs = 100,\n",
        "         batch_size = 10240,\n",
        "         #callbacks = callbacks_list(),\n",
        "         verbose = 2)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "20/20 - 17s - loss: 9.3984 - accuracy: 0.0056\n",
            "Epoch 2/100\n",
            "20/20 - 9s - loss: 7.1926 - accuracy: 0.0401\n",
            "Epoch 3/100\n",
            "20/20 - 9s - loss: 6.8398 - accuracy: 0.0744\n",
            "Epoch 4/100\n",
            "20/20 - 9s - loss: 6.8074 - accuracy: 0.0744\n",
            "Epoch 5/100\n",
            "20/20 - 9s - loss: 6.8010 - accuracy: 0.0744\n",
            "Epoch 6/100\n",
            "20/20 - 9s - loss: 6.7992 - accuracy: 0.0744\n",
            "Epoch 7/100\n",
            "20/20 - 9s - loss: 6.7984 - accuracy: 0.0744\n",
            "Epoch 8/100\n",
            "20/20 - 9s - loss: 6.7976 - accuracy: 0.0744\n",
            "Epoch 9/100\n",
            "20/20 - 9s - loss: 6.7972 - accuracy: 0.0744\n",
            "Epoch 10/100\n",
            "20/20 - 9s - loss: 6.7968 - accuracy: 0.0744\n",
            "Epoch 11/100\n",
            "20/20 - 9s - loss: 6.7959 - accuracy: 0.0744\n",
            "Epoch 12/100\n",
            "20/20 - 9s - loss: 6.7953 - accuracy: 0.0744\n",
            "Epoch 13/100\n",
            "20/20 - 9s - loss: 6.7948 - accuracy: 0.0744\n",
            "Epoch 14/100\n",
            "20/20 - 9s - loss: 6.7933 - accuracy: 0.0744\n",
            "Epoch 15/100\n",
            "20/20 - 9s - loss: 6.7916 - accuracy: 0.0744\n",
            "Epoch 16/100\n",
            "20/20 - 9s - loss: 6.7875 - accuracy: 0.0744\n",
            "Epoch 17/100\n",
            "20/20 - 9s - loss: 6.7765 - accuracy: 0.0744\n",
            "Epoch 18/100\n",
            "20/20 - 9s - loss: 6.7432 - accuracy: 0.0744\n",
            "Epoch 19/100\n",
            "20/20 - 9s - loss: 6.6735 - accuracy: 0.0744\n",
            "Epoch 20/100\n",
            "20/20 - 9s - loss: 6.6259 - accuracy: 0.0744\n",
            "Epoch 21/100\n",
            "20/20 - 9s - loss: 6.5998 - accuracy: 0.0744\n",
            "Epoch 22/100\n",
            "20/20 - 9s - loss: 6.5617 - accuracy: 0.0759\n",
            "Epoch 23/100\n",
            "20/20 - 9s - loss: 6.5030 - accuracy: 0.0935\n",
            "Epoch 24/100\n",
            "20/20 - 9s - loss: 6.4430 - accuracy: 0.0959\n",
            "Epoch 25/100\n",
            "20/20 - 9s - loss: 6.3969 - accuracy: 0.0981\n",
            "Epoch 26/100\n",
            "20/20 - 9s - loss: 6.3642 - accuracy: 0.1008\n",
            "Epoch 27/100\n",
            "20/20 - 9s - loss: 6.3391 - accuracy: 0.1023\n",
            "Epoch 28/100\n",
            "20/20 - 9s - loss: 6.3163 - accuracy: 0.1041\n",
            "Epoch 29/100\n",
            "20/20 - 9s - loss: 6.2935 - accuracy: 0.1059\n",
            "Epoch 30/100\n",
            "20/20 - 9s - loss: 6.2694 - accuracy: 0.1070\n",
            "Epoch 31/100\n",
            "20/20 - 9s - loss: 6.2451 - accuracy: 0.1082\n",
            "Epoch 32/100\n",
            "20/20 - 9s - loss: 6.2197 - accuracy: 0.1088\n",
            "Epoch 33/100\n",
            "20/20 - 9s - loss: 6.1944 - accuracy: 0.1093\n",
            "Epoch 34/100\n",
            "20/20 - 9s - loss: 6.1701 - accuracy: 0.1098\n",
            "Epoch 35/100\n",
            "20/20 - 9s - loss: 6.1473 - accuracy: 0.1102\n",
            "Epoch 36/100\n",
            "20/20 - 9s - loss: 6.1261 - accuracy: 0.1105\n",
            "Epoch 37/100\n",
            "20/20 - 9s - loss: 6.1069 - accuracy: 0.1113\n",
            "Epoch 38/100\n",
            "20/20 - 9s - loss: 6.0885 - accuracy: 0.1124\n",
            "Epoch 39/100\n",
            "20/20 - 9s - loss: 6.0699 - accuracy: 0.1133\n",
            "Epoch 40/100\n",
            "20/20 - 9s - loss: 6.0509 - accuracy: 0.1145\n",
            "Epoch 41/100\n",
            "20/20 - 9s - loss: 6.0281 - accuracy: 0.1173\n",
            "Epoch 42/100\n",
            "20/20 - 9s - loss: 6.0051 - accuracy: 0.1198\n",
            "Epoch 43/100\n",
            "20/20 - 9s - loss: 5.9833 - accuracy: 0.1212\n",
            "Epoch 44/100\n",
            "20/20 - 9s - loss: 5.9628 - accuracy: 0.1223\n",
            "Epoch 45/100\n",
            "20/20 - 9s - loss: 5.9431 - accuracy: 0.1245\n",
            "Epoch 46/100\n",
            "20/20 - 9s - loss: 5.9244 - accuracy: 0.1269\n",
            "Epoch 47/100\n",
            "20/20 - 9s - loss: 5.9041 - accuracy: 0.1289\n",
            "Epoch 48/100\n",
            "20/20 - 9s - loss: 5.8831 - accuracy: 0.1316\n",
            "Epoch 49/100\n",
            "20/20 - 9s - loss: 5.8623 - accuracy: 0.1331\n",
            "Epoch 50/100\n",
            "20/20 - 9s - loss: 5.8413 - accuracy: 0.1345\n",
            "Epoch 51/100\n",
            "20/20 - 9s - loss: 5.8204 - accuracy: 0.1352\n",
            "Epoch 52/100\n",
            "20/20 - 9s - loss: 5.7989 - accuracy: 0.1366\n",
            "Epoch 53/100\n",
            "20/20 - 9s - loss: 5.7788 - accuracy: 0.1376\n",
            "Epoch 54/100\n",
            "20/20 - 9s - loss: 5.7576 - accuracy: 0.1386\n",
            "Epoch 55/100\n",
            "20/20 - 9s - loss: 5.7386 - accuracy: 0.1397\n",
            "Epoch 56/100\n",
            "20/20 - 9s - loss: 5.7201 - accuracy: 0.1402\n",
            "Epoch 57/100\n",
            "20/20 - 9s - loss: 5.7040 - accuracy: 0.1415\n",
            "Epoch 58/100\n",
            "20/20 - 9s - loss: 5.6867 - accuracy: 0.1421\n",
            "Epoch 59/100\n",
            "20/20 - 9s - loss: 5.6723 - accuracy: 0.1424\n",
            "Epoch 60/100\n",
            "20/20 - 9s - loss: 5.6576 - accuracy: 0.1435\n",
            "Epoch 61/100\n",
            "20/20 - 9s - loss: 5.6451 - accuracy: 0.1437\n",
            "Epoch 62/100\n",
            "20/20 - 9s - loss: 5.6323 - accuracy: 0.1441\n",
            "Epoch 63/100\n",
            "20/20 - 9s - loss: 5.6205 - accuracy: 0.1447\n",
            "Epoch 64/100\n",
            "20/20 - 9s - loss: 5.6092 - accuracy: 0.1455\n",
            "Epoch 65/100\n",
            "20/20 - 9s - loss: 5.5976 - accuracy: 0.1456\n",
            "Epoch 66/100\n",
            "20/20 - 9s - loss: 5.5861 - accuracy: 0.1460\n",
            "Epoch 67/100\n",
            "20/20 - 9s - loss: 5.5759 - accuracy: 0.1462\n",
            "Epoch 68/100\n",
            "20/20 - 9s - loss: 5.5663 - accuracy: 0.1468\n",
            "Epoch 69/100\n",
            "20/20 - 9s - loss: 5.5548 - accuracy: 0.1468\n",
            "Epoch 70/100\n",
            "20/20 - 9s - loss: 5.5448 - accuracy: 0.1471\n",
            "Epoch 71/100\n",
            "20/20 - 9s - loss: 5.5346 - accuracy: 0.1476\n",
            "Epoch 72/100\n",
            "20/20 - 9s - loss: 5.5250 - accuracy: 0.1479\n",
            "Epoch 73/100\n",
            "20/20 - 9s - loss: 5.5154 - accuracy: 0.1482\n",
            "Epoch 74/100\n",
            "20/20 - 9s - loss: 5.5041 - accuracy: 0.1485\n",
            "Epoch 75/100\n",
            "20/20 - 9s - loss: 5.4945 - accuracy: 0.1487\n",
            "Epoch 76/100\n",
            "20/20 - 9s - loss: 5.4830 - accuracy: 0.1489\n",
            "Epoch 77/100\n",
            "20/20 - 9s - loss: 5.4712 - accuracy: 0.1493\n",
            "Epoch 78/100\n",
            "20/20 - 9s - loss: 5.4605 - accuracy: 0.1496\n",
            "Epoch 79/100\n",
            "20/20 - 9s - loss: 5.4487 - accuracy: 0.1501\n",
            "Epoch 80/100\n",
            "20/20 - 9s - loss: 5.4344 - accuracy: 0.1503\n",
            "Epoch 81/100\n",
            "20/20 - 9s - loss: 5.4240 - accuracy: 0.1509\n",
            "Epoch 82/100\n",
            "20/20 - 9s - loss: 5.4109 - accuracy: 0.1517\n",
            "Epoch 83/100\n",
            "20/20 - 9s - loss: 5.3980 - accuracy: 0.1523\n",
            "Epoch 84/100\n",
            "20/20 - 9s - loss: 5.3856 - accuracy: 0.1527\n",
            "Epoch 85/100\n",
            "20/20 - 9s - loss: 5.3735 - accuracy: 0.1535\n",
            "Epoch 86/100\n",
            "20/20 - 9s - loss: 5.3608 - accuracy: 0.1539\n",
            "Epoch 87/100\n",
            "20/20 - 9s - loss: 5.3528 - accuracy: 0.1541\n",
            "Epoch 88/100\n",
            "20/20 - 9s - loss: 5.3398 - accuracy: 0.1550\n",
            "Epoch 89/100\n",
            "20/20 - 9s - loss: 5.3265 - accuracy: 0.1557\n",
            "Epoch 90/100\n",
            "20/20 - 9s - loss: 5.3148 - accuracy: 0.1560\n",
            "Epoch 91/100\n",
            "20/20 - 9s - loss: 5.3012 - accuracy: 0.1565\n",
            "Epoch 92/100\n",
            "20/20 - 9s - loss: 5.2909 - accuracy: 0.1574\n",
            "Epoch 93/100\n",
            "20/20 - 9s - loss: 5.2775 - accuracy: 0.1578\n",
            "Epoch 94/100\n",
            "20/20 - 9s - loss: 5.2657 - accuracy: 0.1584\n",
            "Epoch 95/100\n",
            "20/20 - 9s - loss: 5.2542 - accuracy: 0.1589\n",
            "Epoch 96/100\n",
            "20/20 - 9s - loss: 5.2406 - accuracy: 0.1593\n",
            "Epoch 97/100\n",
            "20/20 - 9s - loss: 5.2312 - accuracy: 0.1601\n",
            "Epoch 98/100\n",
            "20/20 - 9s - loss: 5.2182 - accuracy: 0.1605\n",
            "Epoch 99/100\n",
            "20/20 - 9s - loss: 5.2050 - accuracy: 0.1613\n",
            "Epoch 100/100\n",
            "20/20 - 9s - loss: 5.1920 - accuracy: 0.1622\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fda4db03590>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deEZABPy-cZS",
        "outputId": "9fa8e020-86bc-4422-edbf-2008ddb45c15"
      },
      "source": [
        "# fit model\n",
        "model.fit(np.asarray(trainX), pd.get_dummies(np.asarray(trainy)), batch_size=128, epochs=100)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1571/1571 [==============================] - 42s 27ms/step - loss: 5.6572 - accuracy: 0.1438\n",
            "Epoch 2/100\n",
            "1571/1571 [==============================] - 42s 27ms/step - loss: 5.3876 - accuracy: 0.1534\n",
            "Epoch 3/100\n",
            "1571/1571 [==============================] - 42s 27ms/step - loss: 5.2467 - accuracy: 0.1583\n",
            "Epoch 4/100\n",
            "1571/1571 [==============================] - 42s 27ms/step - loss: 5.1154 - accuracy: 0.1627\n",
            "Epoch 5/100\n",
            "1571/1571 [==============================] - 41s 26ms/step - loss: 4.9842 - accuracy: 0.1677\n",
            "Epoch 6/100\n",
            "1571/1571 [==============================] - 41s 26ms/step - loss: 4.8589 - accuracy: 0.1726\n",
            "Epoch 7/100\n",
            "1571/1571 [==============================] - 41s 26ms/step - loss: 4.7435 - accuracy: 0.1776\n",
            "Epoch 8/100\n",
            "1571/1571 [==============================] - 41s 26ms/step - loss: 4.6356 - accuracy: 0.1832\n",
            "Epoch 9/100\n",
            "1571/1571 [==============================] - 41s 26ms/step - loss: 4.5390 - accuracy: 0.1893\n",
            "Epoch 10/100\n",
            "1571/1571 [==============================] - 42s 27ms/step - loss: 4.4493 - accuracy: 0.1966\n",
            "Epoch 11/100\n",
            "1571/1571 [==============================] - 42s 27ms/step - loss: 4.3683 - accuracy: 0.2020\n",
            "Epoch 12/100\n",
            "1571/1571 [==============================] - 42s 27ms/step - loss: 4.2923 - accuracy: 0.2091\n",
            "Epoch 13/100\n",
            "1571/1571 [==============================] - 42s 27ms/step - loss: 4.2240 - accuracy: 0.2149\n",
            "Epoch 14/100\n",
            "1571/1571 [==============================] - 42s 27ms/step - loss: 4.1593 - accuracy: 0.2211\n",
            "Epoch 15/100\n",
            "1571/1571 [==============================] - 42s 27ms/step - loss: 4.0977 - accuracy: 0.2274\n",
            "Epoch 16/100\n",
            "1571/1571 [==============================] - 42s 27ms/step - loss: 4.0373 - accuracy: 0.2328\n",
            "Epoch 17/100\n",
            "1571/1571 [==============================] - 42s 27ms/step - loss: 3.9819 - accuracy: 0.2389\n",
            "Epoch 18/100\n",
            "1571/1571 [==============================] - 42s 27ms/step - loss: 3.9283 - accuracy: 0.2456\n",
            "Epoch 19/100\n",
            "1571/1571 [==============================] - 42s 27ms/step - loss: 3.8745 - accuracy: 0.2516\n",
            "Epoch 20/100\n",
            "1571/1571 [==============================] - 42s 27ms/step - loss: 3.8221 - accuracy: 0.2578\n",
            "Epoch 21/100\n",
            "1571/1571 [==============================] - 42s 27ms/step - loss: 3.7750 - accuracy: 0.2620\n",
            "Epoch 22/100\n",
            "1571/1571 [==============================] - 42s 27ms/step - loss: 3.7229 - accuracy: 0.2693\n",
            "Epoch 23/100\n",
            "1571/1571 [==============================] - 42s 27ms/step - loss: 3.6773 - accuracy: 0.2747\n",
            "Epoch 24/100\n",
            "1571/1571 [==============================] - 42s 27ms/step - loss: 3.6290 - accuracy: 0.2807\n",
            "Epoch 25/100\n",
            "1571/1571 [==============================] - 42s 27ms/step - loss: 3.5831 - accuracy: 0.2862\n",
            "Epoch 26/100\n",
            "1571/1571 [==============================] - 42s 27ms/step - loss: 3.5395 - accuracy: 0.2925\n",
            "Epoch 27/100\n",
            "1571/1571 [==============================] - 42s 27ms/step - loss: 3.4955 - accuracy: 0.2977\n",
            "Epoch 28/100\n",
            "1571/1571 [==============================] - 42s 27ms/step - loss: 3.4519 - accuracy: 0.3030\n",
            "Epoch 29/100\n",
            "1571/1571 [==============================] - 41s 26ms/step - loss: 3.4138 - accuracy: 0.3087\n",
            "Epoch 30/100\n",
            "1571/1571 [==============================] - 41s 26ms/step - loss: 3.3681 - accuracy: 0.3149\n",
            "Epoch 31/100\n",
            "1571/1571 [==============================] - 41s 26ms/step - loss: 3.3277 - accuracy: 0.3198\n",
            "Epoch 32/100\n",
            "1571/1571 [==============================] - 41s 26ms/step - loss: 3.2902 - accuracy: 0.3255\n",
            "Epoch 33/100\n",
            "1571/1571 [==============================] - 41s 26ms/step - loss: 3.2499 - accuracy: 0.3309\n",
            "Epoch 34/100\n",
            "1571/1571 [==============================] - 41s 26ms/step - loss: 3.2123 - accuracy: 0.3364\n",
            "Epoch 35/100\n",
            "1571/1571 [==============================] - 41s 26ms/step - loss: 3.1721 - accuracy: 0.3416\n",
            "Epoch 36/100\n",
            "1571/1571 [==============================] - 41s 26ms/step - loss: 3.1352 - accuracy: 0.3484\n",
            "Epoch 37/100\n",
            "1571/1571 [==============================] - 41s 26ms/step - loss: 3.0994 - accuracy: 0.3529\n",
            "Epoch 38/100\n",
            "1571/1571 [==============================] - 41s 26ms/step - loss: 3.0647 - accuracy: 0.3576\n",
            "Epoch 39/100\n",
            "1571/1571 [==============================] - 41s 26ms/step - loss: 3.0305 - accuracy: 0.3628\n",
            "Epoch 40/100\n",
            "1571/1571 [==============================] - 41s 26ms/step - loss: 2.9942 - accuracy: 0.3690\n",
            "Epoch 41/100\n",
            "1571/1571 [==============================] - 41s 26ms/step - loss: 2.9574 - accuracy: 0.3740\n",
            "Epoch 42/100\n",
            "1571/1571 [==============================] - 40s 26ms/step - loss: 2.9236 - accuracy: 0.3786\n",
            "Epoch 43/100\n",
            "1571/1571 [==============================] - 40s 26ms/step - loss: 2.8934 - accuracy: 0.3836\n",
            "Epoch 44/100\n",
            "1571/1571 [==============================] - 40s 26ms/step - loss: 2.8587 - accuracy: 0.3888\n",
            "Epoch 45/100\n",
            "1571/1571 [==============================] - 40s 26ms/step - loss: 2.8264 - accuracy: 0.3954\n",
            "Epoch 46/100\n",
            "1571/1571 [==============================] - 40s 26ms/step - loss: 2.7953 - accuracy: 0.3989\n",
            "Epoch 47/100\n",
            "1571/1571 [==============================] - 40s 26ms/step - loss: 2.7612 - accuracy: 0.4043\n",
            "Epoch 48/100\n",
            "1571/1571 [==============================] - 40s 26ms/step - loss: 2.7336 - accuracy: 0.4083\n",
            "Epoch 49/100\n",
            "1571/1571 [==============================] - 40s 26ms/step - loss: 2.7002 - accuracy: 0.4142\n",
            "Epoch 50/100\n",
            "1571/1571 [==============================] - 41s 26ms/step - loss: 2.6706 - accuracy: 0.4190\n",
            "Epoch 51/100\n",
            "1571/1571 [==============================] - 40s 26ms/step - loss: 2.6440 - accuracy: 0.4241\n",
            "Epoch 52/100\n",
            "1571/1571 [==============================] - 40s 26ms/step - loss: 2.6114 - accuracy: 0.4283\n",
            "Epoch 53/100\n",
            "1571/1571 [==============================] - 41s 26ms/step - loss: 2.5847 - accuracy: 0.4329\n",
            "Epoch 54/100\n",
            "1571/1571 [==============================] - 41s 26ms/step - loss: 2.5553 - accuracy: 0.4395\n",
            "Epoch 55/100\n",
            "1571/1571 [==============================] - 41s 26ms/step - loss: 2.5282 - accuracy: 0.4434\n",
            "Epoch 56/100\n",
            "1571/1571 [==============================] - 41s 26ms/step - loss: 2.4966 - accuracy: 0.4488\n",
            "Epoch 57/100\n",
            "1571/1571 [==============================] - 40s 26ms/step - loss: 2.4676 - accuracy: 0.4530\n",
            "Epoch 58/100\n",
            "1571/1571 [==============================] - 41s 26ms/step - loss: 2.4431 - accuracy: 0.4567\n",
            "Epoch 59/100\n",
            "1571/1571 [==============================] - 41s 26ms/step - loss: 2.4184 - accuracy: 0.4619\n",
            "Epoch 60/100\n",
            "1571/1571 [==============================] - 41s 26ms/step - loss: 2.3924 - accuracy: 0.4659\n",
            "Epoch 61/100\n",
            "1571/1571 [==============================] - 41s 26ms/step - loss: 2.3645 - accuracy: 0.4709\n",
            "Epoch 62/100\n",
            "1571/1571 [==============================] - 41s 26ms/step - loss: 2.3367 - accuracy: 0.4766\n",
            "Epoch 63/100\n",
            "1571/1571 [==============================] - 41s 26ms/step - loss: 2.3160 - accuracy: 0.4788\n",
            "Epoch 64/100\n",
            "1571/1571 [==============================] - 41s 26ms/step - loss: 2.2872 - accuracy: 0.4844\n",
            "Epoch 65/100\n",
            "1571/1571 [==============================] - 41s 26ms/step - loss: 2.2674 - accuracy: 0.4883\n",
            "Epoch 66/100\n",
            "1571/1571 [==============================] - 41s 26ms/step - loss: 2.2410 - accuracy: 0.4923\n",
            "Epoch 67/100\n",
            "1571/1571 [==============================] - 41s 26ms/step - loss: 2.2208 - accuracy: 0.4966\n",
            "Epoch 68/100\n",
            "1571/1571 [==============================] - 42s 26ms/step - loss: 2.1938 - accuracy: 0.5015\n",
            "Epoch 69/100\n",
            "1571/1571 [==============================] - 42s 27ms/step - loss: 2.1692 - accuracy: 0.5063\n",
            "Epoch 70/100\n",
            "1571/1571 [==============================] - 42s 27ms/step - loss: 2.1492 - accuracy: 0.5099\n",
            "Epoch 71/100\n",
            "1571/1571 [==============================] - 42s 27ms/step - loss: 2.1272 - accuracy: 0.5134\n",
            "Epoch 72/100\n",
            "1571/1571 [==============================] - 42s 27ms/step - loss: 2.1054 - accuracy: 0.5179\n",
            "Epoch 73/100\n",
            "1571/1571 [==============================] - 42s 27ms/step - loss: 2.0870 - accuracy: 0.5212\n",
            "Epoch 74/100\n",
            "1571/1571 [==============================] - 42s 27ms/step - loss: 2.0657 - accuracy: 0.5262\n",
            "Epoch 75/100\n",
            "1571/1571 [==============================] - 42s 27ms/step - loss: 2.0426 - accuracy: 0.5294\n",
            "Epoch 76/100\n",
            "1571/1571 [==============================] - 42s 27ms/step - loss: 2.0188 - accuracy: 0.5341\n",
            "Epoch 77/100\n",
            "1571/1571 [==============================] - 42s 27ms/step - loss: 2.0034 - accuracy: 0.5371\n",
            "Epoch 78/100\n",
            "1571/1571 [==============================] - 42s 27ms/step - loss: 1.9865 - accuracy: 0.5400\n",
            "Epoch 79/100\n",
            "1571/1571 [==============================] - 42s 27ms/step - loss: 1.9671 - accuracy: 0.5440\n",
            "Epoch 80/100\n",
            "1571/1571 [==============================] - 42s 27ms/step - loss: 1.9446 - accuracy: 0.5491\n",
            "Epoch 81/100\n",
            "1571/1571 [==============================] - 42s 27ms/step - loss: 1.9266 - accuracy: 0.5516\n",
            "Epoch 82/100\n",
            "1571/1571 [==============================] - 42s 27ms/step - loss: 1.9066 - accuracy: 0.5563\n",
            "Epoch 83/100\n",
            "1571/1571 [==============================] - 42s 27ms/step - loss: 1.8939 - accuracy: 0.5583\n",
            "Epoch 84/100\n",
            "1571/1571 [==============================] - 42s 26ms/step - loss: 1.8710 - accuracy: 0.5628\n",
            "Epoch 85/100\n",
            "1571/1571 [==============================] - 42s 27ms/step - loss: 1.8542 - accuracy: 0.5652\n",
            "Epoch 86/100\n",
            "1571/1571 [==============================] - 41s 26ms/step - loss: 1.8384 - accuracy: 0.5684\n",
            "Epoch 87/100\n",
            "1571/1571 [==============================] - 41s 26ms/step - loss: 1.8237 - accuracy: 0.5710\n",
            "Epoch 88/100\n",
            "1571/1571 [==============================] - 41s 26ms/step - loss: 1.8030 - accuracy: 0.5761\n",
            "Epoch 89/100\n",
            "1571/1571 [==============================] - 41s 26ms/step - loss: 1.7868 - accuracy: 0.5786\n",
            "Epoch 90/100\n",
            "1571/1571 [==============================] - 42s 26ms/step - loss: 1.7705 - accuracy: 0.5823\n",
            "Epoch 91/100\n",
            "1571/1571 [==============================] - 42s 26ms/step - loss: 1.7592 - accuracy: 0.5839\n",
            "Epoch 92/100\n",
            "1571/1571 [==============================] - 41s 26ms/step - loss: 1.7406 - accuracy: 0.5874\n",
            "Epoch 93/100\n",
            "1571/1571 [==============================] - 42s 27ms/step - loss: 1.7258 - accuracy: 0.5906\n",
            "Epoch 94/100\n",
            "1571/1571 [==============================] - 43s 27ms/step - loss: 1.7136 - accuracy: 0.5919\n",
            "Epoch 95/100\n",
            "1571/1571 [==============================] - 42s 27ms/step - loss: 1.6938 - accuracy: 0.5974\n",
            "Epoch 96/100\n",
            "1571/1571 [==============================] - 42s 27ms/step - loss: 1.6823 - accuracy: 0.5994\n",
            "Epoch 97/100\n",
            "1571/1571 [==============================] - 42s 27ms/step - loss: 1.6638 - accuracy: 0.6029\n",
            "Epoch 98/100\n",
            "1571/1571 [==============================] - 41s 26ms/step - loss: 1.6561 - accuracy: 0.6042\n",
            "Epoch 99/100\n",
            "1571/1571 [==============================] - 42s 26ms/step - loss: 1.6415 - accuracy: 0.6078\n",
            "Epoch 100/100\n",
            "1571/1571 [==============================] - 42s 26ms/step - loss: 1.6204 - accuracy: 0.6132\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fda24323a50>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmCsBpeNF26G"
      },
      "source": [
        "pickle.dump(tokenizer, open('tokenizer.pkl', 'wb'))\n",
        "model.save('model_weights.hdf5')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QU4LZUrCba5s"
      },
      "source": [
        "def gen(model,seq,max_len = 20):\n",
        "    ''' Generates a sequence given a string seq using specified model until the total sequence length\n",
        "    reaches max_len'''\n",
        "    # Tokenize the input string\n",
        "    tokenized_sent = tokenizer.texts_to_sequences([seq])\n",
        "    max_len = max_len+len(tokenized_sent[0])\n",
        "    # If sentence is not as long as the desired sentence length, we need to 'pad sequence' so that\n",
        "    # the array input shape is correct going into our LSTM. the `pad_sequences` function adds \n",
        "    # zeroes to the left side of our sequence until it becomes 19 long, the number of input features.\n",
        "    while len(tokenized_sent[0]) < max_len:\n",
        "        padded_sentence = pad_sequences(tokenized_sent[-19:],maxlen=19)\n",
        "        op = model.predict(np.asarray(padded_sentence).reshape(1,-1))\n",
        "        tokenized_sent[0].append(op.argmax()+1)\n",
        "        \n",
        "    return \" \".join(map(lambda x : reverse_word_map[x],tokenized_sent[0]))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R06aIotEcIb7"
      },
      "source": [
        "model_list = [model]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqRGhk7wbgWZ"
      },
      "source": [
        "\n",
        "def test_models(test_string,sequence_length= 50, model_list = model_list):\n",
        "    '''Generates output given input test_string up to sequence_length'''\n",
        "    print('Input String: ', test_string)\n",
        "    for counter,model in enumerate(model_list):\n",
        "        print(\"Model \", counter+1, \":\")\n",
        "        print(gen(model,test_string,sequence_length))\n",
        "    pass"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFuLqs3Yb62m",
        "outputId": "5be9ce78-74e1-4cdf-8c24-d55ce078a63d"
      },
      "source": [
        "\n",
        "test_models('This process however afforded me', 10)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input String:  This process however afforded me\n",
            "Model  1 :\n",
            "this process however afforded me in the smallest i shall say that the thief is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13KSWgYAcXdD",
        "outputId": "7fa14c60-7e5d-4b44-c680-8116b6954e1e"
      },
      "source": [
        "test_models(author.iloc[3709])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input String:  what avails the vigilance against the Destiny of man?\n",
            "Model  1 :\n",
            "what avails the vigilance against the destiny of man inclined to the angels nor bear about buried to have something to excite the lee or so moreover not the division been west but the natives suspend it by a cord seldom have not a lamp and then i was enabled to confess the women about seen the beetle here\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbzhFkbJcabR",
        "outputId": "e08e4022-16dc-4d47-c348-d4782740378c"
      },
      "source": [
        "\n",
        "test_models(author.iloc[58],50)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input String:  By these means for they were ignorant men I found little difficulty in gaining them over to my purpose.\n",
            "Model  1 :\n",
            "by these means for they were ignorant men i found little difficulty in gaining them over to my purpose with much suffering afterward in connexion with my medical windows i interposed the frenchman with which the deity expressed even he was the only ones of the same large and thoughtful frequented in the title of the bishop of chichester stay for me to be johannisberger now the medium week\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "un2OIhiIcfUe",
        "outputId": "a6658f00-d2b9-44fe-adbc-317078534479"
      },
      "source": [
        "test_models(author.iloc[70])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input String:  In the meantime it was folly to grieve, or to think.\n",
            "Model  1 :\n",
            "in the meantime it was folly to grieve or to think of the intruder in the ordinary character in the world to be sure was generally is was making to revolve at slack and as i put in the street place or re metzengerstein i am not seven employed during the midway who were so well express that the prefect knows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FaRI1SgcjOY",
        "outputId": "e6692532-c6bb-421c-e0f5-4757b4550a06"
      },
      "source": [
        "test_models(author.iloc[7800])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input String:  First of all I dismembered the corpse.\n",
            "Model  1 :\n",
            "first of all i dismembered the corpse in the money itself from the library well had never visited or knocked firmly away up de peg throw the parchment a charitable and had been supposed to me the original object of its moral and and social spots sustain energy if i had been preferred that as i confided\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0M_EQnecnS6",
        "outputId": "d2411386-7aca-4158-8978-06ffea5c00df"
      },
      "source": [
        "test_models(author.iloc[7120])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input String:  Here, however, are Moissart, Voissart, Croissart, and Froissart, all in the direct line of descent.\n",
            "Model  1 :\n",
            "here however are moissart voissart croissart and froissart all in the direct line of descent the whole story that is law the whole philosophy knows his birds always dangerous extent i turned the spectacles and so collected its escape at length and dismissing me with similes it in the essence the task of years or of many weeks or fish a wild and quaint ground\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0x5G5y--cqp8",
        "outputId": "9ef4f5ff-08f3-4e0b-d11e-b8c8d208185b"
      },
      "source": [
        "test_models(author.iloc[5121])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input String:  \"There are two windows in the chamber.\n",
            "Model  1 :\n",
            "there are two windows in the chamber which oppressed me than the beast the nerve conveys luminous similar and harsh and money and puppy melancholy the articles had been deviates from the wings of nature by the wailings and in the silk of the bizarre overshadowed the greatest from the earth's familiar tones of the atmosphere is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1r_VChscuR2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}